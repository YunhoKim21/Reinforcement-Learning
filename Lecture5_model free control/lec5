Monte-carlo policy iteration 

polict evaluation : monte-carlo policy evaluation(just doing a lot of times), Q=q_pi

policy improvement = epsilon-greedy policy improvement 
why epsilon-greedy, not just greedy? 
-> because greedy might miss a better reward

how to make it more efficient 
 we dont need the fully correct value function
극단적으로 monte-carlo를 한번만!!! 실행하고 그에 따른 입실론 그리디를 하는 것도 고려 가능 -> 정확한 Q function을 알지 못해도 괜찮다는 장점이 있음


GLIE : greedy in the limit with infinite exploration
1. all state -action pairs are explored infinitely many times 

2. the polict converges on a greedy policy

for example, epsilon-greedy is GLIE if epsilon reduces to zero at c_k = 1/k
